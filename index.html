<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation. A lightweight post-training framework that aligns autoregressive visual generators with a principled pixel-space objective.">
  <meta name="theme-color" content="#f7f8fb">

  <!-- Open Graph -->
  <meta property="og:type" content="website">
  <meta property="og:title" content="VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation">
  <meta property="og:description" content="A lightweight post-training framework that aligns autoregressive visual generators with a principled pixel-space objective.">
  <meta property="og:image" content="static/img/teaser.png">
  <meta property="og:url" content="https://va-pi.github.io/">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation">
  <meta name="twitter:description" content="A lightweight post-training framework that aligns autoregressive visual generators with a principled pixel-space objective.">
  <meta name="twitter:image" content="static/img/teaser.png">

  <link rel="canonical" href="https://va-pi.github.io/">
  <link rel="icon" href="static/img/favicon.svg" type="image/svg+xml">
  <title>VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</title>
  <link rel="stylesheet" href="static/css/style.css">
  <script defer src="static/js/main.js"></script>
</head>
<body>
  <div class="paper" id="top">
    <!-- Sticky TOC (desktop) -->
    <nav class="toc navlinks" aria-label="Table of contents">
      <div class="toc-title">Contents</div>
      <a href="#overview">Overview</a>
      <a href="#observations">Observations</a>
      <a href="#quant-results-ablation">Quantitative Results (C2I/T2I, Ablation)</a>
      <a href="#quant-results">Quantitative Results (C2I/T2I)</a>
    </nav>

    <header class="paper-header">
      <div class="paper-header-inner">
        <h1 class="paper-title">VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</h1>

        <div class="paper-authors">
          <a href="https://lil-shake.github.io/" target="_blank" rel="noopener">Xinyao Liao</a><sup>*1</sup>,
          <a href="https://qy-h00.github.io/" target="_blank" rel="noopener">Qiyuan He</a><sup>*†2</sup>,
          <a href="https://kai422.github.io/" target="_blank" rel="noopener">Kai Xu</a><sup>2</sup>,
          <a href="https://scholar.google.com/citations?user=rT3hqdcAAAAJ&hl=zh-CN" target="_blank" rel="noopener">Xiaoye Qu</a><sup>1</sup>,
          <a href="https://yl3800.github.io/" target="_blank" rel="noopener">Yicong Li</a><sup>2</sup>,
          <a href="https://www.eric-weiwei.com/" target="_blank" rel="noopener">Wei Wei</a><sup>1</sup>,
          <a href="https://www.comp.nus.edu.sg/~ayao/" target="_blank" rel="noopener">Angela Yao</a><sup>2</sup>
        </div>

        <div class="paper-affiliations">
          <span><sup>1</sup>Huazhong University of Science and Technology</span>
          <span><sup>2</sup>National University of Singapore</span>
        </div>

        <div class="paper-contrib">
          <span><sup>*</sup>Equal contribution</span>
          <span><sup>†</sup>Project lead</span>
        </div>

        <div class="paper-badges" aria-label="Project links">
          <a class="badge secondary disabled" href="#" aria-disabled="true">arXiv (coming soon)</a>
          <a class="badge" href="https://github.com/Lil-Shake/VA-Pi" target="_blank" rel="noopener">Code</a>
        </div>

        <figure class="paper-teaser" aria-label="Teaser">
          <img src="static/img/vis-c2i-t2i_00.png" alt="VA-π teaser">
        </figure>

        <div class="tldr-section">
          <p><span class="tldr-label">TL;DR:</span> How should visual autoregressive models be optimized?
            <b>Token space or pixel space?</b> We argue the answer is pixel space.
            We propose <b>VA-π</b>, a lightweight post-training framework that directly optimizes <b>visual AR models</b> with a principled <b>pixel-space objective</b>!
            <!-- , solved by introducing <b>evidence lower bound (ELBO)</b> that unifies pixel reconstruction and autoregressive modeling.
            To optimize under the discrete token space, VA-π introduces a <b>RL-based alignment strategy</b> that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward and Next Token Prediction (NTP) loss with noisy context as regulizer. -->
          </p>
        </div>
      </div>
    </header>

    <main class="paper-main" id="main">
      <section class="paper-section" id="overview">
        <h2>Overview</h2>
        <p>
          <!-- TODO: replace with final paper overview -->
          Autoregressive (AR) visual generation relies on tokenizers to map images to and from <b>discrete sequences</b>.  However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for <b>token likelihood</b>. This misalignment leads to generated token sequences that may decode into <b>low-quality images</b>, without direct supervision from the pixel space. 

          Specifically, we propose <b>VA-π</b>, a lightweight post-training framework that directly optimizes AR models with a principled <b>pixel-space objective</b>. VA-π formulates the generator–tokenizer alignment as a variational optimization, deriving an <b>evidence lower bound (ELBO)</b> that unifies <b>pixel reconstruction and autoregressive modeling</b>. To optimize under the discrete token space, VA-π introduces a <b>RL-based alignment strategy</b> that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. 
        </p>
        <figure class="paper-figure" aria-label="Method overview">
          <img src="static/img/method.png" alt="Method Overview">
          <figcaption>Overview of the VA-π method.</figcaption>
        </figure>
      </section>

      <section class="paper-section" id="observations">
        <h2>Observations</h2>

        <h3 id="obs-alignment-to-pixel-space">1. Alignment to Pixel-Space</h3>
        <p>
          <!-- TODO: replace with final text -->
          VA-π enables efficient post-training via variational policy optimization, aligning the pixel-space distribution of AR generated images with that of ground-truth images.
        </p>
        <figure class="paper-figure" aria-label="Observation 1 teaser">
          <img src="static/img/obs-1.png" alt="Alignment to Pixel Space teaser">
          <figcaption>Visualization of VA-π promoting alignment from token space generations back to the pixel space.</figcaption>
        </figure>

        <h3 id="obs-better-than-naive">2. VA-π is better than naive post-train</h3>
        <p>
          <!-- TODO: replace with final text -->
          We compare VA-π with naive post-training baselines, showing more stable optimization and improved pixel-space fidelity.
        </p>
        <div class="figure-placeholder" aria-label="Observation 2 figure placeholder">
          <div class="placeholder-note">[Place comparisons vs naive post-train here]</div>
        </div>
      </section>

      <section class="paper-section" id="quant-results-ablation">
        <h2>Quantitative Results (C2I/T2I, Ablation Study)</h2>
        <p class="section-note">
          <!-- TODO -->
          Report ablations for both class-to-image (C2I) and text-to-image (T2I) settings.
        </p>
        <div class="table-placeholder" aria-label="Ablation tables placeholder">
          <div class="placeholder-note">[Place C2I + T2I ablation tables here]</div>
        </div>
      </section>

      <section class="paper-section" id="quant-results">
        <h2>Quantitative Results (C2I/T2I)</h2>
        <p class="section-note">
          <!-- TODO -->
          Main quantitative results for both C2I and T2I benchmarks.
        </p>
        <div class="table-placeholder" aria-label="Main results tables placeholder">
          <div class="placeholder-note">[Place C2I + T2I main results tables/plots here]</div>
        </div>
      </section>

      <section class="paper-section" id="citation">
        <h2>Citation</h2>
        <p class="note">Update the BibTeX below once the paper/arXiv entry is finalized.</p>
        <div class="codebox">
          <div class="codebox-head">
            <div class="codebox-title">BibTeX</div>
            <button class="copy-btn" type="button" data-copy-target="#bibtex">Copy</button>
          </div>
          <pre class="codeblock"><code id="bibtex">@article{vapi2025,
  title   = {VA-\\pi: Variational Policy Alignment for Pixel-Aware Autoregressive Generation},
  author  = {Liao, Xinyao and He, Qiyuan and Xu, Kai and Qu, Xiaoye and Li, Yicong and Wei, Wei and Yao, Angela},
  journal = {arXiv preprint arXiv:????.?????},
  year    = {2025}
}</code></pre>
        </div>
      </section>
    </main>
  </div>
</body>
</html>
