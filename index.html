<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation. A lightweight post-training framework that aligns autoregressive visual generators with a principled pixel-space objective.">
  <meta name="theme-color" content="#f7f8fb">

  <!-- Open Graph -->
  <meta property="og:type" content="website">
  <meta property="og:title" content="VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation">
  <meta property="og:description" content="A lightweight post-training framework that aligns autoregressive visual generators with a principled pixel-space objective.">
  <meta property="og:image" content="static/img/teaser.png">
  <meta property="og:url" content="https://va-pi.github.io/">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation">
  <meta name="twitter:description" content="A lightweight post-training framework that aligns autoregressive visual generators with a principled pixel-space objective.">
  <meta name="twitter:image" content="static/img/teaser.png">

  <link rel="canonical" href="https://va-pi.github.io/">
  <link rel="icon" href="static/img/favicon.svg" type="image/svg+xml">
  <title>VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</title>
  <link rel="stylesheet" href="static/css/style.css">
  <script defer src="static/js/main.js"></script>
</head>
<body>
  <div class="paper" id="top">
    <!-- Sticky TOC (desktop) -->
    <nav class="toc navlinks" aria-label="Table of contents">
      <div class="toc-title">Contents</div>
      <a href="#overview">Overview</a>
      <a href="#observations">Observations</a>
      <a href="#quant-results-ablation">Ablation Study</a>
      <a href="#quant-results">More Qualitative Results</a>
    </nav>

    <header class="paper-header">
      <div class="paper-header-inner">
        <h1 class="paper-title">VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</h1>

        <div class="paper-authors">
          <a href="https://lil-shake.github.io/" target="_blank" rel="noopener">Xinyao Liao</a><sup>*1</sup>,
          <a href="https://qy-h00.github.io/" target="_blank" rel="noopener">Qiyuan He</a><sup>*†2</sup>,
          <a href="https://kai422.github.io/" target="_blank" rel="noopener">Kai Xu</a><sup>2</sup>,
          <a href="https://scholar.google.com/citations?user=rT3hqdcAAAAJ&hl=zh-CN" target="_blank" rel="noopener">Xiaoye Qu</a><sup>1</sup>,
          <a href="https://yl3800.github.io/" target="_blank" rel="noopener">Yicong Li</a><sup>2</sup>,
          <a href="https://www.eric-weiwei.com/" target="_blank" rel="noopener">Wei Wei</a><sup>1</sup>,
          <a href="https://www.comp.nus.edu.sg/~ayao/" target="_blank" rel="noopener">Angela Yao</a><sup>2</sup>
        </div>

        <div class="paper-affiliations">
          <span><sup>1</sup>Huazhong University of Science and Technology</span>
          <span><sup>2</sup>National University of Singapore</span>
        </div>

        <div class="paper-contrib">
          <span><sup>*</sup>Equal contribution</span>
          <span><sup>†</sup>Project lead</span>
        </div>

        <div class="paper-badges" aria-label="Project links">
          <a class="badge" href="https://arxiv.org/abs/2512.19680" target="_blank" rel="noopener" aria-label="arXiv paper">
            <svg class="badge-icon" viewBox="0 0 24 24" aria-hidden="true" focusable="false">
              <path fill="currentColor" d="M6 2h9l3 3v17H6V2zm9 1.5V6h2.5L15 3.5z"></path>
              <path fill="currentColor" d="M8 10h8v2H8v-2zm0 4h8v2H8v-2z"></path>
            </svg>
            <span>arXiv</span>
          </a>
          <a class="badge" href="https://github.com/Lil-Shake/VA-Pi" target="_blank" rel="noopener" aria-label="GitHub repository">
            <svg class="badge-icon" viewBox="0 0 24 24" aria-hidden="true" focusable="false">
              <path fill="currentColor" d="M12 .5C5.73.5.5 5.74.5 12.2c0 5.16 3.44 9.53 8.21 11.08.6.11.82-.26.82-.58v-2.02c-3.34.73-4.04-1.61-4.04-1.61-.55-1.39-1.34-1.76-1.34-1.76-1.09-.74.08-.73.08-.73 1.2.09 1.84 1.25 1.84 1.25 1.07 1.84 2.8 1.31 3.49 1 .11-.78.42-1.31.76-1.61-2.67-.31-5.47-1.35-5.47-6 0-1.33.47-2.42 1.24-3.27-.12-.31-.54-1.57.12-3.27 0 0 1.01-.33 3.3 1.25a11.3 11.3 0 0 1 3-.41c1.02 0 2.05.14 3 .41 2.28-1.58 3.29-1.25 3.29-1.25.66 1.7.24 2.96.12 3.27.77.85 1.23 1.94 1.23 3.27 0 4.66-2.8 5.69-5.48 6 .43.37.81 1.11.81 2.24v3.32c0 .32.22.69.82.58 4.77-1.55 8.2-5.92 8.2-11.08C23.5 5.74 18.27.5 12 .5z"></path>
            </svg>
            <span>GitHub</span>
          </a>
        </div>

        <figure class="paper-teaser" aria-label="Teaser">
          <img src="static/img/vis-c2i-t2i_00.png" alt="VA-π teaser">
        </figure>

        <div class="tldr-section">
          <p><span class="tldr-label">TL;DR:</span> How should visual autoregressive models be optimized?
            <b>Token space or pixel space?</b> We argue the answer is pixel space.
            We propose <b>VA-π</b>, a lightweight post-training framework that directly optimizes <b>visual autoregressive models</b> with a principled <b>pixel-space objective</b>!
            <!-- , solved by introducing <b>evidence lower bound (ELBO)</b> that unifies pixel reconstruction and autoregressive modeling.
            To optimize under the discrete token space, VA-π introduces a <b>RL-based alignment strategy</b> that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward and Next Token Prediction (NTP) loss with noisy context as regulizer. -->
          </p>
        </div>
      </div>
    </header>

    <main class="paper-main" id="main">
      <section class="paper-section" id="overview">
        <h2>Overview</h2>
        <p>
          <!-- TODO: replace with final paper overview -->
          Autoregressive (AR) visual generation relies on tokenizers to map images to and from <b>discrete sequences</b>.  However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for <b>token likelihood</b>. This misalignment leads to generated token sequences that may decode into <b>low-quality images</b>, without direct supervision from the pixel space. 

          Specifically, we propose <b>VA-π</b>, a lightweight post-training framework that directly optimizes AR models with a principled <b>pixel-space objective</b>. VA-π formulates the generator–tokenizer alignment as a variational optimization, deriving an <b>evidence lower bound (ELBO)</b> that unifies <b>pixel reconstruction and autoregressive modeling</b>. To optimize under the discrete token space, VA-π introduces a <b>RL-based alignment strategy</b> that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. 
        </p>
        <figure class="paper-figure figure-full" aria-label="Method overview">
          <img src="static/img/method.png" alt="Method Overview" loading="lazy" decoding="async">
          <figcaption>Overview of the VA-π method.</figcaption>
        </figure>
      </section>

      <section class="paper-section" id="observations">
        <h2>Observations</h2>

        <h3 id="obs-alignment-to-pixel-space">1. Alignment to pixel-space</h3>
        <p>
          <!-- TODO: replace with final text -->
          VA-π aligns autoregressive image generation with the ground-truth distribution in pixel space. Qualitatively, VA-π corrects off-manifold token sequences that decode into distorted structures, producing more coherent and faithful reconstructions. Quantitatively, both <b>embedding density estimation (KDE)</b> and <b>low-dimensional projections (t-SNE)</b> show that VA-π shifts generated images closer to the ground-truth manifold.
        </p>
        <figure class="paper-figure figure-full" aria-label="Observation 1 teaser">
          <img src="static/img/obs-1.png" alt="Alignment to Pixel Space teaser" loading="lazy" decoding="async">
          <figcaption>VA-π brings autoregressive generations closer to the ground-truth image manifold in both appearance and distribution.</figcaption>
        </figure>

        <h3 id="obs-better-than-naive">2. VA-π is better than naive post-train</h3>
        <p>
          We benchmark VA-π against naive post-training baselines on two representative settings:
          <b>class-to-image</b> generation on <b>ImageNet-1K</b> (LlamaGen-XL/XXL; evaluated with and without CFG),
          and <b>text-to-image</b> reasoning on <b>GenEval</b> (LlamaGen-XL and the unified multimodal model Janus-Pro 1B).
          We report standard generation metrics (FID/IS/Precision/Recall for C2I; GenEval sub-scores and overall for T2I) and
          include tuning time and whether an <b>external reward</b> is used.
        </p>
        <ul class="bullet-highlights">
          <li><b>Lightweight gains.</b> Without external reward, VA-π improves LlamaGen-XXL on ImageNet-1K from <b>FID 14.36 → 7.65</b> (w/o CFG) in <b>25 minutes</b>, and improves LlamaGen-XL from <b>FID 15.55 → 9.23</b> in <b>20 minutes</b>.</li>
          <li><b>Better quality at low cost.</b> VA-π boosts perceptual quality significantly: for LlamaGen-XXL (w/o CFG) <b>IS 86.55 → 116.70</b>, and for LlamaGen-XL (w/o CFG) <b>IS 79.16 → 111.59</b>; with CFG it reaches <b>IS 299.63</b> on LlamaGen-XL while still keeping tuning time at <b>20 minutes</b>.</li>
           <li><b>Generalizes beyond C2I.</b> On GenEval, VA-π improves LlamaGen-XL overall from <b>0.306 → 0.339</b>, and improves the unified multimodal model Janus-Pro 1B from <b>0.725 → 0.744</b>, without task-specific fine-tuning on GenEval.</li>
        </ul>

        <figure class="paper-table" aria-label="C2I quantitative results table">
          <div class="table-scroll" role="region" aria-label="Scrollable table">
            <table>
              <thead>
                <tr>
                  <th rowspan="2" style="text-align:left;">Model</th>
                  <th rowspan="2">Ext. Rwd</th>
                  <th rowspan="2">Time (min) ↓</th>
                  <th colspan="4">w/o cfg</th>
                  <th colspan="4">w/ cfg</th>
                </tr>
                <tr>
                  <th>FID ↓</th><th>IS ↑</th><th>Pre. ↑</th><th>Rec. ↑</th>
                  <th>FID ↓</th><th>IS ↑</th><th>Pre. ↑</th><th>Rec. ↑</th>
                </tr>
              </thead>
              <tbody>
                <tr class="row-group">
                  <td style="text-align:left;"><b>LlamaGen-XL (775M)</b></td>
                  <td>--</td><td>--</td>
                  <td>15.55</td><td>79.16</td><td>0.62</td><td>0.69</td>
                  <td class="hl">2.79</td><td>286.88</td><td>0.84</td><td>0.54</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;+ AR-GRPO</td>
                  <td>✓</td><td>149</td>
                  <td>--</td><td>--</td><td>--</td><td>--</td>
                  <td>3.63</td><td>293.07</td><td>0.86</td><td>0.48</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;+ <b>VA-π (Ours)</b></td>
                  <td>×</td><td>20</td>
                  <td class="hl">9.23</td><td class="hl">111.59</td><td>0.71</td><td>0.59</td>
                  <td>2.94</td><td class="hl">299.63</td><td>0.84</td><td>0.53</td>
                </tr>

                <tr class="sep"><td colspan="11"></td></tr>

                <tr class="row-group">
                  <td style="text-align:left;"><b>LlamaGen-XXL (1.4B)</b></td>
                  <td>--</td><td>--</td>
                  <td>14.36</td><td>86.55</td><td>0.63</td><td>0.69</td>
                  <td>2.37</td><td>252.16</td><td>0.81</td><td>0.59</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;+ Post-train Tokenizer</td>
                  <td>×</td><td>18</td>
                  <td>14.26</td><td>86.70</td><td>0.63</td><td>0.68</td>
                  <td>2.72</td><td>246.97</td><td>0.80</td><td>0.59</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;+ Post-train Tokenizer (longer)</td>
                  <td>×</td><td>207</td>
                  <td>22.99</td><td>72.49</td><td>0.56</td><td>0.68</td>
                  <td>4.31</td><td>221.57</td><td>0.75</td><td>0.58</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;+ STE based Post-train AR</td>
                  <td>×</td><td>381</td>
                  <td>11.46</td><td>102.21</td><td>0.68</td><td>0.61</td>
                  <td>4.17</td><td>267.34</td><td>0.83</td><td>0.51</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;+ <b>VA-π (Ours)</b></td>
                  <td>×</td><td>25</td>
                  <td class="hl">7.65</td><td class="hl">116.70</td><td>0.71</td><td>0.64</td>
                  <td class="hl">2.28</td><td class="hl">273.53</td><td>0.83</td><td>0.56</td>
                </tr>
              </tbody>
            </table>
          </div>
          <figcaption>C2I: ImageNet-1K class-conditional quantitative results.</figcaption>
        </figure>

        <figure class="paper-table" aria-label="T2I GenEval results table">
          <div class="table-scroll" role="region" aria-label="Scrollable table">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">Model</th>
                  <th>Ext. Rwd</th>
                  <th>Position ↑</th>
                  <th>Color ↑</th>
                  <th>Attr. Bind. ↑</th>
                  <th>Counting ↑</th>
                  <th>Single Obj. ↑</th>
                  <th>Two Obj. ↑</th>
                  <th>Overall ↑</th>
                </tr>
              </thead>
              <tbody>
                <tr class="row-group">
                  <td style="text-align:left;"><b>LlamaGen-XL</b></td>
                  <td>--</td>
                  <td>0.042</td><td>0.550</td><td>0.032</td><td>0.197</td><td>0.750</td><td>0.263</td><td>0.306</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;+ AR-GRPO</td>
                  <td>✓</td>
                  <td>0.040</td><td>0.593</td><td>0.030</td><td>0.228</td><td class="hl">0.791</td><td>0.263</td><td>0.324</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;+ <b>VA-π (Ours)</b></td>
                  <td>×</td>
                  <td class="hl">0.050</td><td class="hl">0.606</td><td class="hl">0.040</td><td class="hl">0.238</td><td>0.769</td><td class="hl">0.328</td><td class="hl">0.339</td>
                </tr>

                <tr class="sep"><td colspan="9"></td></tr>

                <tr class="row-group">
                  <td style="text-align:left;"><b>Janus-Pro 1B</b></td>
                  <td>-</td>
                  <td class="hl">0.605</td><td>0.902</td><td>0.540</td><td>0.531</td><td>0.972</td><td>0.801</td><td>0.725</td>
                </tr>
                <tr>
                  <td style="text-align:left;">&nbsp;&nbsp;+ <b>VA-π (Ours)</b></td>
                  <td>×</td>
                  <td>0.600</td><td class="hl">0.912</td><td class="hl">0.585</td><td class="hl">0.540</td><td class="hl">0.988</td><td class="hl">0.835</td><td class="hl">0.744</td>
                </tr>
              </tbody>
            </table>
          </div>
          <figcaption>T2I: GenEval quantitative results.</figcaption>
        </figure>

        <p>
          In addition to quantitative improvements, we provide a qualitative comparison to highlight the visual differences among naive post-training baselines and <b>VA-π</b>.
          Under identical decoding settings (<b>ImageNet-1K</b>, <b>CFG = 1.0</b>), VA-π produces samples with more coherent structure and fewer token-induced artifacts.
        </p>

        <figure class="paper-figure figure-half" aria-label="C2I qualitative comparison among post-training methods">
          <img src="static/img/vis-pt-ste_00.png" alt="C2I qualitative comparison: LlamaGen-XXL vs PT vs STE vs VA-π" loading="lazy" decoding="async">
          <figcaption>
            C2I qualitative comparison (ImageNet-1K, CFG = 1.0): LlamaGen-XXL vs post-train tokenizer (PT) vs STE post-train AR vs VA-π.
          </figcaption>
        </figure>

        <p class="section-note">
          For additional qualitative comparisons (C2I and T2I), see <a href="#quant-results">More Qualitative Results</a>.
        </p>
      </section>

      <section class="paper-section" id="quant-results-ablation">
        <h2>Ablation Study</h2>
        <p>
          We ablate key components in <b>VA-π</b>, including <b>reward composition</b>, <b>prior regularization</b>, and <b>contextual noise</b>.
        </p>

        <h3 id="ablation-reward">Reward and Loss Composition</h3>
        <p>
          Reconstruction-only rewards (<span class="mono">L</span><sub>MSE</sub>/<span class="mono">L</span><sub>p</sub>) are unstable because the policy drifts from the pre-trained token distribution.
          Adding token-level prior regularization (<span class="mono">L</span><sub>prior</sub>, cross-entropy) stabilizes training, and the full objective achieves the best overall trade-off.
        </p>

        <figure class="paper-table" aria-label="Ablation on reward composition (w/o CFG)">
          <div class="table-scroll" role="region" aria-label="Scrollable table">
            <table>
              <thead>
                <tr>
                  <th><span class="mono">L</span><sub>MSE</sub></th>
                  <th><span class="mono">L</span><sub>p</sub></th>
                  <th><span class="mono">L</span><sub>prior</sub></th>
                  <th><b>FID ↓</b></th>
                  <th><b>IS ↑</b></th>
                  <th><b>Pre. ↑</b></th>
                  <th><b>Rec. ↑</b></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td></td><td></td><td></td>
                  <td>14.36</td><td>86.55</td><td>0.63</td><td>0.69</td>
                </tr>
                <tr>
                  <td></td><td>✓</td><td></td>
                  <td>38.76</td><td>49.78</td><td>0.48</td><td>0.46</td>
                </tr>
                <tr>
                  <td>✓</td><td>✓</td><td></td>
                  <td>38.63</td><td>48.14</td><td>0.49</td><td>0.46</td>
                </tr>
                <tr>
                  <td></td><td></td><td>✓</td>
                  <td>14.17</td><td>88.78</td><td>0.63</td><td>0.69</td>
                </tr>
                <tr>
                  <td>✓</td><td>✓</td><td>✓</td>
                  <td class="hl"><b>7.65</b></td><td class="hl"><b>116.70</b></td><td>0.68</td><td>0.64</td>
                </tr>
              </tbody>
            </table>
          </div>
          <figcaption>
            <b>Ablation on reward composition (w/o CFG).</b>
            We analyze the contribution of each reward component:
            <span class="mono">L</span><sub>MSE</sub> (pixel-level reconstruction),
            <span class="mono">L</span><sub>p</sub> (perceptual similarity via LPIPS),
            and <span class="mono">L</span><sub>prior</sub> (token-level cross-entropy regularization).
          </figcaption>
        </figure>

        <h3 id="ablation-regularization">Prior Regularization Term</h3>
        <p>
          We vary the regularization weight (&beta;) for <b>KL</b> vs <b>CE</b> variants.
          Without regularization, optimization diverges (FID 38.63); moderate regularization (&beta; = 0.1) improves both FID and IS, while too-strong regularization (&beta; = 1.0) hurts diversity.
          <b>CE</b> consistently outperforms <b>KL</b>, with the best results at &beta; = 0.1.
        </p>

        <figure class="paper-figure figure-full" aria-label="Ablation on regularization weight (w/o CFG)">
          <div class="figure-row">
            <div>
              <img src="static/img/reg_ablation_fid.png" alt="FID over regularization weight beta" loading="lazy" decoding="async">
              <div class="figure-subcap">(a) FID over weight &beta;</div>
            </div>
            <div>
              <img src="static/img/reg_ablation_is.png" alt="IS over regularization weight beta" loading="lazy" decoding="async">
              <div class="figure-subcap">(b) IS over weight &beta;</div>
            </div>
          </div>
          <figcaption>
            <b>Ablation on regularization weight (w/o CFG).</b>
            CE regularization consistently outperforms KL regularization on FID and IS. Moderate CE regularization (&beta; = 0.1) provides the best results.
          </figcaption>
        </figure>

        <h3 id="ablation-noise">Contextual Noise</h3>
        <p>
          We ablate the corruption probability (&xi;) for contextual noise in the LlamaGen T2I post-train setting.
          Moderate noise (&xi; = 0.5) performs best on GenEval (Overall 0.339), while &xi; = 0 or overly strong noise (&xi; &gt; 0.75) degrades performance.
        </p>

        <figure class="paper-table" aria-label="Ablation on noise ratio (xi) during training">
          <div class="table-scroll" role="region" aria-label="Scrollable table">
            <table>
              <thead>
                <tr>
                  <th><b>&xi;</b></th>
                  <th><b>PT</b></th>
                  <th><b>CL</b></th>
                  <th><b>AB</b></th>
                  <th><b>CT</b></th>
                  <th><b>SO</b></th>
                  <th><b>TO</b></th>
                  <th><b>Overall</b></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>0</td><td>0.048</td><td>0.566</td><td>0.023</td><td>0.159</td><td>0.688</td><td>0.326</td><td>0.302</td>
                </tr>
                <tr>
                  <td>0.25</td><td>0.043</td><td>0.598</td><td>0.025</td><td>0.215</td><td>0.700</td><td>0.306</td><td>0.315</td>
                </tr>
                <tr>
                  <td>0.5</td><td class="hl"><b>0.050</b></td><td class="hl"><b>0.606</b></td><td class="hl"><b>0.040</b></td><td class="hl"><b>0.238</b></td><td class="hl"><b>0.769</b></td><td class="hl"><b>0.328</b></td><td class="hl"><b>0.339</b></td>
                </tr>
                <tr>
                  <td>0.75</td><td>0.075</td><td>0.641</td><td>0.028</td><td>0.163</td><td>0.750</td><td>0.333</td><td>0.332</td>
                </tr>
                <tr>
                  <td>0.95</td><td>0.043</td><td>0.652</td><td>0.040</td><td>0.181</td><td>0.728</td><td>0.328</td><td>0.329</td>
                </tr>
              </tbody>
            </table>
          </div>
          <figcaption>
            <b>Ablation on noise ratio (&xi;) during training.</b>
            Moderate noise ratio (0.5) achieves the best overall performance on GenEval.
            Abbreviations: PT (Position), CL (Color), AB (Attribute Binding), CT (Counting), SO (Single Object), TO (Two objects).
          </figcaption>
        </figure>
      </section>

      <section class="paper-section" id="quant-results">
        <h2>More Qualitative Results</h2>

        <h3>Class-to-image generation (ImageNet-1K)</h3>
        <p>
          We provide additional qualitative comparisons on C2I generation across ImageNet-1K classes.
          All samples use identical decoding settings (<b>CFG = 1.0</b>, temperature = 1.0, top-k = 0, top-p = 1.0).
        </p>

        <div class="qual-grid" aria-label="Additional ImageNet class qualitative comparisons">
          <figure class="paper-figure">
          <img src="static/img/021_kite_comparison_grid-cfg-1.0.png" alt="Qualitative comparison on the kite class" loading="lazy" decoding="async">
            <figcaption>ImageNet C2I: <span class="mono">kite</span>.</figcaption>
          </figure>
          <figure class="paper-figure">
          <img src="static/img/167_English_foxhound_comparison_grid-cfg-1.0.png" alt="Qualitative comparison on the English foxhound class" loading="lazy" decoding="async">
            <figcaption>ImageNet C2I: <span class="mono">English foxhound</span>.</figcaption>
          </figure>
          <figure class="paper-figure">
          <img src="static/img/285_Egyptian_cat_comparison_grid-cfg-1.0.png" alt="Qualitative comparison on the Egyptian cat class" loading="lazy" decoding="async">
            <figcaption>ImageNet C2I: <span class="mono">Egyptian cat</span>.</figcaption>
          </figure>
          <figure class="paper-figure">
          <img src="static/img/417_balloon_comparison_grid-cfg-1.0.png" alt="Qualitative comparison on the balloon class" loading="lazy" decoding="async">
            <figcaption>ImageNet C2I: <span class="mono">balloon</span>.</figcaption>
          </figure>
          <figure class="paper-figure">
          <img src="static/img/538_dome_comparison_grid-cfg-1.0.png" alt="Qualitative comparison on the dome class" loading="lazy" decoding="async">
            <figcaption>ImageNet C2I: <span class="mono">dome</span>.</figcaption>
          </figure>
          <figure class="paper-figure">
          <img src="static/img/696_paintbrush_comparison_grid-cfg-1.0.png" alt="Qualitative comparison on the paintbrush class" loading="lazy" decoding="async">
            <figcaption>ImageNet C2I: <span class="mono">paintbrush</span>.</figcaption>
          </figure>
          <figure class="paper-figure">
          <img src="static/img/856_thresher__thrasher__threshing_machine_comparison_grid-cfg-1.0.png" alt="Qualitative comparison on the thresher / thrasher / threshing machine class" loading="lazy" decoding="async">
            <figcaption>ImageNet C2I: <span class="mono">thresher / thrasher / threshing machine</span>.</figcaption>
          </figure>
          <figure class="paper-figure">
          <img src="static/img/933_cheeseburger_comparison_grid-cfg-1.0.png" alt="Qualitative comparison on the cheeseburger class" loading="lazy" decoding="async">
            <figcaption>ImageNet C2I: <span class="mono">cheeseburger</span>.</figcaption>
          </figure>
          <figure class="paper-figure">
          <img src="static/img/953_pineapple__ananas_comparison_grid-cfg-1.0.png" alt="Qualitative comparison on the pineapple / ananas class" loading="lazy" decoding="async">
            <figcaption>ImageNet C2I: <span class="mono">pineapple / ananas</span>.</figcaption>
          </figure>
          <figure class="paper-figure">
          <img src="static/img/997_bolete_comparison_grid-cfg-1.0.png" alt="Qualitative comparison on the bolete class" loading="lazy" decoding="async">
            <figcaption>ImageNet C2I: <span class="mono">bolete</span>.</figcaption>
          </figure>
        </div>

        <h3>Text-to-image generation (GenEval)</h3>
        <p>
          We present additional T2I qualitative comparisons on GenEval prompts, focusing on harder compositional tasks
          (attribute binding, counting, position, two-object combination). All samples use identical decoding settings
          (<b>CFG = 5.0</b>, temperature = 1.0, top-k = 0, top-p = 1.0).
        </p>

        <div class="qual-grid" aria-label="Additional GenEval qualitative comparisons">
          <figure class="paper-figure">
          <img src="static/img/Janus-Pro_1B_comparison_grid_category_color_attr_4x4.png" alt="Qualitative comparison on attribute binding" loading="lazy" decoding="async">
            <figcaption>GenEval: <span class="mono">attribute binding</span>.</figcaption>
          </figure>
          <figure class="paper-figure">
          <img src="static/img/Janus-Pro_1B_comparison_grid_category_counting_4x4.png" alt="Qualitative comparison on counting" loading="lazy" decoding="async">
            <figcaption>GenEval: <span class="mono">counting</span>.</figcaption>
          </figure>
          <figure class="paper-figure">
          <img src="static/img/Janus-Pro_1B_comparison_grid_category_position_4x4.png" alt="Qualitative comparison on position" loading="lazy" decoding="async">
            <figcaption>GenEval: <span class="mono">position</span>.</figcaption>
          </figure>
          <figure class="paper-figure">
          <img src="static/img/Janus-Pro_1B_comparison_grid_category_two_object_4x4.png" alt="Qualitative comparison on two-object combination" loading="lazy" decoding="async">
            <figcaption>GenEval: <span class="mono">two-object combination</span>.</figcaption>
          </figure>
        </div>
      </section>

      <section class="paper-section" id="citation">
        <h2>Citation</h2>
        <!-- <p class="note">Update the BibTeX below once the paper/arXiv entry is finalized.</p> -->
        <div class="codebox">
          <div class="codebox-head">
            <div class="codebox-title">BibTeX</div>
            <button class="copy-btn" type="button" data-copy-target="#bibtex">Copy</button>
          </div>
          <pre class="codeblock"><code id="bibtex">@misc{vapi2025,
            title={VA-$\pi$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation}, 
            author={Xinyao Liao and Qiyuan He and Kai Xu and Xiaoye Qu and Yicong Li and Wei Wei and Angela Yao},
            year={2025},
            eprint={2512.19680},
            archivePrefix={arXiv},
            primaryClass={cs.CV},
            url={https://arxiv.org/abs/2512.19680}, 
      }</code></pre>
        </div>
      </section>
    </main>
  </div>
</body>
</html>
